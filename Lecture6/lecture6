% ---------- Титульный слайд -------------
\title{Статистический анализ данных. Спецкурс.}
\subtitle{Лекция 6. Методы многомерной статистики}
\author{Ботанический сад-институт ДВО РАН}% \\ \texttt{kislov@easydan.com}}
\date{Кислов Д.Е. \\ \today}
\maketitle
% -----------------------------------------



% --------------- слайд 2 -----------------
\begin{frame}
\frametitle{Структура лекции}
\begin{itemize}
\item<1-> Принцип наименьших квадратов;
\item<2-> Метод главных компонент;
\item<3-> Линейный дискриминантный анализ;
\item<4-> Классификация по прецедентам;
\item<5-> Оценка качества классификации, отбор признаков.
\end{itemize}
\end{frame}
% -----------------------------------------


% --------------- слайд 3 -----------------
\begin{frame}
\frametitle{Принцип наименьших квадратов}
\begin{columns}
	\begin{column}{5cm}
		Предложен К.Ф. Гауссом (1795) для решения уравнений:
		$$
		\begin{array}{l}
			a_1x+b_1y = c_1\only<2->{+\varepsilon_1}\\
			a_2x+b_2y = c_2\only<2->{+\varepsilon_2}\\
			a_3x+b_3y = c_3\only<2->{+\varepsilon_3}\\
			\only<2->{
			\varepsilon_1^2+\varepsilon_2^2+\varepsilon_3^2\rightarrow\min
			}
		\end{array}
		$$
	\end{column}
	\begin{column}{5cm}
	Задача регрессии
	\includegraphics[width=5cm]{mnk.png}
	\end{column}
\end{columns}
\end{frame}
% -----------------------------------------

% --------------- слайд 4 -----------------
\begin{frame}
\frametitle{Принцип наименьших квадратов}
\begin{exampleblock}{Применение}
	\begin{itemize}
		\item<1-> Решение переопределенных/недоопределенных систем уравнений;
		\item<2-> Статистическая оценка параметров;
		\item<3-> Решение задач снижения размерности;
		\item<4-> Построение регрессионных моделей;
		\item<5-> \ldots в общем случае --- любые другие задачи,
			связанные с минимизацией ошибок/погрешностей.
	\end{itemize}
\end{exampleblock}
\end{frame}
% -----------------------------------------

% --------------- слайд 5 -----------------
\begin{frame}
\frametitle{Принцип наименьших квадратов}
\begin{exampleblock}{Задача регрессии}
	Имеется набор измерений $y_j$ ($j=\overline{1,\ldots,N}$, 
	предположительно зависящий от параметров $x_{ij}$ ($i=\overline{1,\ldots,M}$).
	Необходимо построить какую-либо модель этой зависимости, исходя из
	набора эмпирических данных.
\end{exampleblock}
\only<2->{
\begin{alertblock}{Положим,что зависимость между $y_i$ и $x_{ij}$ линейная}
\end{alertblock}}
\only<3->{
	$$\begin{array}{l}
	y_1 = a_0 + a_1\cdot x_{11} + a_2\cdot x_{21}+\ldots + a_M\cdot x_{M1} \only<4->{+\varepsilon_1} \\
	y_2 = a_0 + a_1\cdot x_{12} + a_2\cdot x_{22}+\ldots + a_M\cdot x_{M2} \only<4->{+\varepsilon_2} \\
	\vdots \\
	y_N = a_0 + a_1\cdot x_{1N} + a_2\cdot x_{2N}+\ldots + a_M\cdot x_{MN} 
	\only<4->{+\varepsilon_N}  \\
	\end{array}
	$$
	\only<4->{$$\sum\limits_i\varepsilon_i^2\rightarrow\min$$}}
\end{frame}
% -----------------------------------------

% --------------- слайд 6 -----------------
\begin{frame}
\frametitle{Принцип наименьших квадратов}
\begin{exampleblock}{Решение}
$$
\begin{array}{l}
	Y = X\cdot a +\varepsilon, a=(a_0,a_1,\ldots,a_M),\\
	a = (X^TX)^{-1}XY, \mbox{или } a = X^{+}Y
\end{array}
$$
\end{exampleblock}
\begin{block}{Нелинейный МНК}
	$$y_j = F(a_i,x_{ij})+\varepsilon_j$$
\end{block}
\only<2->{
\begin{alertblock}{Решение}
	Как правило -- численные методы:
	нелинейные методы оптимизации, проблемно-ориентированные подходы.

	А также \ldots
	Существуют частные случаи -- легко приводимые к линейной задаче.
\end{alertblock}}
\end{frame}
% -----------------------------------------

% --------------- слайд 7 -----------------
\begin{frame}
\frametitle{Метод главных компонент}
\only<1>{
\begin{exampleblock}{Формулировка задачи}
	\begin{columns}
		\begin{column}{5cm}
		\includegraphics[width=5cm]{pca.jpg}
		\end{column}
		\begin{column}{5cm}
		Отыскать такую ось -- линейную комбинацию исходных координат, сумма квадратов расстояний 
		от данных до которой минимальна;
		
		(метод предложен К. Пирсоном);
		\end{column}
	\end{columns}
\end{exampleblock}
}
\only<2->{
\begin{alertblock}{Интерпретации}
	\begin{itemize}
		\item<2-> Теория вероятностей -- диагонализация ковариационной матрицы (преобразование Хотеллинга);
		\item<3-> Статистика -- максимизация вариации (дисперсии) проекций данных на прямую;
		\item<4-> Механика -- отыскание главных осей инерции;
	\end{itemize}
\end{alertblock}}
\end{frame}
% -----------------------------------------

% --------------- слайд 8 -----------------
\begin{frame}
\frametitle{Метод главных компонент}
\begin{exampleblock}{Вычислительные аспекты}
	Пусть $X$ --- исходная матрица данных (имеющих нулевое среднее); 
	$S = \frac{1}{N-1}X^TX$ -- выборочная ковариационная матрица. 
	Тогда решение задачи отыскания главных компонент определяется ее спектральным разложением
	$$
	S = U^T \cdot\Sigma U,
	$$
	где $U$ -- ортогональная матрица; $\Sigma=(\lambda_1,\ldots,\lambda_2)$.
\end{exampleblock}
\begin{block}{}
	$$
	\begin{array}{l}
	tr(\Sigma) = \sigma_1^2+\sigma_2^2+\ldots+\sigma_m^2 \\ 
	\mu_k = \frac{\lambda_1+\ldots+\lambda_k}{\sum\limits_i \lambda_i}
	\end{array}
	$$
\end{block}
\end{frame}
% -----------------------------------------

% --------------- слайд 9 -----------------
\begin{frame}
\frametitle{Классификация по прецедентам}
\begin{block}{Формулировка задачи на примере классификации ирисов}
\begin{center}\includegraphics[width=9cm]{suplearning.jpg}\end{center}
\end{block}


\end{frame}
% -----------------------------------------

% --------------- слайд 10 -----------------
\begin{frame}
\frametitle{Классификация по прецедентам}
\begin{block}{Формулировка задачи на примере классификации рукописных цифр}
\begin{center}
	\includegraphics[width=4cm]{hwd.jpg} $\rightarrow$
	\includegraphics[width=2cm]{hwdigitized.png} 
\end{center}
\end{block}
\only<2->{
\begin{alertblock}{!}
	Реальные задачи распознавания образов могут иметь очень большие размерности.
\end{alertblock}}
\end{frame}
% -----------------------------------------

% --------------- слайд 11 -----------------
\begin{frame}
\frametitle{Основные этапы решения задач классификации по прецедентам}
\begin{itemize}
	\item<1-> Подготовка данных: приведение к единому масштабу (при необходимости), центрирование, удаление выбросов и т.п.
	\item<2-> Отбор и формирование переменных (features engeneering, features extraction, features selection );
	\item<3-> Выбор классификатора и подстройка его параметров;
	\item<4-> Тестирование классификатора (при необходимости -- повторение операций с п. 2).
\end{itemize}
\end{frame}
% -----------------------------------------

% --------------- слайд 12 -----------------
\begin{frame}
\frametitle{Методы решения задач классификации }
\begin{itemize}
	\item<1-> Наивный Байесовский классификатор; 
	\item<2-> Метод k-ближайших соседей;
	\item<3-> Линейная и квадратичная дискриминация;
	\item<4-> Деревья решений;
	\item<5-> Машины опорных векторов;
	\item<6-> Нейросетевые классификаторы;
	\item<7-> Комбинирование классификаторов;
\end{itemize}
\end{frame}
% -----------------------------------------


